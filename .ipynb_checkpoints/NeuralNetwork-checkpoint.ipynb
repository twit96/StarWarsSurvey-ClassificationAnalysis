{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis of Star Wars Survey Dataset\n",
    "\n",
    "#### Chance Mason, Nicolas Arrieche Villegas, Mitchell Walker, Tyler Wittig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "# 4.4 Neural Network Classifier \n",
    "\n",
    "In this notebook we will implement a Neural Network Classifier to classify a survey participant as either a fan of Star wars (1) or Not a fan of Star Wars (-1). We will discuss our decision to focus on predicting only this label in more detail later on.\n",
    "\n",
    "We used the SciKit Learn object `MLPClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV\n",
    "We first read the CSV of numerical survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1186, 37)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = r\"C:\\Users\\User\\AppData\\Local\\Packages\\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\\LocalState\\rootfs\\home\\mitchel\\StarWarsSurvey-ClassificationAnalysis\\survey_numeric.csv\"\n",
    "\n",
    "data = pd.read_csv(filepath)\n",
    "data.head(15)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Cleaning / Feature Engineering\n",
    "\n",
    "Because we are trying to predict whether or not a respondent is a fan of Star Wars or not, we remove all record where the respondent answered \"no\" to the question \"Seen a Star Wars film\", as the remainder of their answers will be null or carry very little identifying information.\n",
    "\n",
    "We then drop the features \"Location (Census Region)\" and \"Seen a Star Wars film\" entirely, because after the model was trained, it was found that the weights from these specific input nodes to the first hidden layer were almost entirely zero. This implies that these feature values had very little effect on the models classification, as large negative or positive values imply the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all records where respondent has not seen a star wars film\n",
    "data.drop(data[data[\"Seen a Star Wars film\"] == 0].index,inplace=True)\n",
    "\n",
    "# drop location column and \"Seen a star wars film\"\n",
    "data = data.drop([\"Location (Census Region)\"], axis=1)\n",
    "data = data.drop([\"Seen a Star Wars film\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Features Columns from Labels\n",
    "Here we split the data on the given label.\n",
    "\n",
    "We create a function that takes the label as an argument, and returns featurs (data_X) and labels (data_Y). This functions allows for the flexibilty to easily train the model to predict different labels (Fan of Star Wars, Star Trek Fan, Gender, Age, Household Income, and Education), without changing much of the code.\n",
    "\n",
    "We remove the records in which the label value is zero. For the labels mentioned above the value of zero represents and Unknown or Null response, which is not a response that we should expect our model to accurately predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fan of Star Wars\n",
      "Seen The Phantom Menace\n",
      "Seen Attack of the Clones\n",
      "Seen Revenge of the Sith\n",
      "Seen A New Hope\n",
      "Seen The Empire Strikes Back\n",
      "Seen Return of the Jedi\n",
      "Rank for The Phantom Menace\n",
      "Rank for Attack of the Clones\n",
      "Rank for Revenge of the Sith\n",
      "Rank for A New Hope\n",
      "Rank for The Empire Strikes Back\n",
      "Rank for Return of the Jedi\n",
      "View of Han Solo\n",
      "View of Luke Skywalker\n",
      "View of Princess Leia Organa\n",
      "View of Anakin Skywalker\n",
      "View of Obi Wan Kenobi\n",
      "View of Emperor Palpatine\n",
      "View of Darth Vader\n",
      "View of Lando Calrissian\n",
      "View of Boba Fett\n",
      "View of C-3P0\n",
      "View of R2 D2\n",
      "View of Jar Jar Binks\n",
      "View of Padme Amidala\n",
      "View of Yoda\n",
      "Which character shot first?\n",
      "Familiar with the Expanded Universe?\n",
      "Fan of the Expanded Universe?\n",
      "Star Trek Fan\n",
      "Gender\n",
      "Age\n",
      "Household Income\n",
      "Education\n"
     ]
    }
   ],
   "source": [
    "# print all column names\n",
    "for col in data.columns:\n",
    "    print(col)\n",
    "\n",
    "# this function splits the data by features and labels, given the label argument\n",
    "# it returns two objects: data_X and data_Y\n",
    "def splitData(label):\n",
    "    # make copy of data to keep original data unaffected\n",
    "    data_copy = data\n",
    "    \n",
    "    # drop null values\n",
    "    data_copy.drop(data_copy[data_copy[label] == 0].index, inplace=True)\n",
    "    \n",
    "    # split and return data\n",
    "    data_X = data_copy.loc[:,data_copy.columns != label]\n",
    "    data_Y = data_copy[label]\n",
    "    return data_X, data_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Set Up\n",
    "\n",
    "##### GridSearchCV hyperparamters\n",
    "In order to tune the hyperparameters of the Neural Network, we set up a pipeline which scales the data, and pass this into a GridSearchCV object with varying ranges of hyperparameters.\n",
    "We ran this Grid Search over all activation functions. We also initialized the hidden layer sizes to be a single layer, ranging from 10 nodes to 60 nodes, counting by 10.\n",
    "After fitting many Grid Searches, we consistently found that the best parameeters were the largest hidden layer sizes. To push keep searching for the optimal hidden layer geometry, we iteratively adjusted the `hidden_layer_list` to hold larger and larger values. We also tested on two and three hidden layers of similar sizes.\n",
    "\n",
    "##### Converging on optimal hidden layers\n",
    "We saw that the model's optimal hidden layers were consistently a single hidden layer with 110 nodes. We then iteratively made our step size smaller, and closed our range around the most optimal number of hidden layers. Finally, we found that our model's optimal hidden layer geometry was 106 nodes in a single layer, and kept the Grid Search's parameters to search for the best hyperparameters between 100 and 110 (with a step size of 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# define function to create GridSearchCV object\n",
    "\n",
    "scaler = StandardScaler()\n",
    "MLPC = MLPClassifier(solver = 'lbfgs')\n",
    "\n",
    "pipe = Pipeline([(\"Scaler\",scaler),(\"MLPC\",MLPC)])\n",
    "\n",
    "# create list of hidden_layer sizes for param_grid\n",
    "hidden_layer_list = []\n",
    "for i in range(100,111, 2):\n",
    "    hidden_layer_list += [(i,)]\n",
    "\n",
    "    \n",
    "# parameter grid for different hyperparamters in gridsearchCV\n",
    "param_grid = {'MLPC__hidden_layer_sizes':hidden_layer_list,\n",
    "             'MLPC__activation':['identity','logistic','tanh','relu']\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model / Reporting Performance\n",
    "##### Help Me, 'Fan of Star Wars' label: You're My Only Hope\n",
    "Earlier we discussed training the model to predict several labels, but here we decided to only train it on the label \"Fan of Star Wars\". This is for two reasons. First, \"Fan of Star Wars\" and \"Star Trek Fan\" were the only two of the mentioned labels that are bivariate. The others have 5 or 6 options to their survey responses, which means that at random, a classifier could guess these with 16-20% accuracy. Our trained classifiers were guessing these at around 40%, which is good considering the classification against randomness, but bad in general. \n",
    "Additionally, the Neural Network was unable to surpass 68% accuracy for the label \"Star Trek fan\", so we decided to only try to train and predict the label \"Fan of Star Wars\"\n",
    "##### GridSearchCV and Reporting\n",
    "We ran a 5 fold cross validation loop around the 5 fold Grid Search to report on the accuracy of a trained GridSearchCV. We reported on the accuracy, precision, recall, (and FScore).\n",
    "This model had an accuracy of 82.3%, a 1 classification f-score of 87%, and a -1 classification f-score of 72%. This means that more of the 1 classifications were classified correctly than -1, and that of all correct classifications, 1 classes were classified at a higher proportion. This is in part due to the class imbalance between the two. Nevertheless, precision and recall for the -1 classes were 76 and 68% respectively, which is not terrible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 82.30%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[194  90]\n",
      " [ 60 492]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.68      0.72       284\n",
      "           1       0.85      0.89      0.87       552\n",
      "\n",
      "    accuracy                           0.82       836\n",
      "   macro avg       0.80      0.79      0.79       836\n",
      "weighted avg       0.82      0.82      0.82       836\n",
      "\n",
      "{'MLPC__activation': 'identity', 'MLPC__hidden_layer_sizes': (100,)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Create GridSearchCV Object\n",
    "MLPSearch= GridSearchCV(pipe,\n",
    "                  param_grid,\n",
    "                  scoring = 'accuracy',\n",
    "                  cv=5)\n",
    "\n",
    "# display accuracy and confusion matrix for the GridSearchCV\n",
    "data_X, data_Y = splitData('Fan of Star Wars')\n",
    "\n",
    "# Print Accuracy\n",
    "acc = cross_val_score(MLPSearch, data_X, data_Y, cv=5)\n",
    "print(\"Average Accuracy: {:.2f}%\\n\".format(acc.mean()*100))\n",
    "\n",
    "# cross_val_predict\n",
    "y_pred = cross_val_predict(MLPSearch, data_X, data_Y)\n",
    "\n",
    "# print confusion matrix\n",
    "conf_mat = confusion_matrix(data_Y, y_pred)\n",
    "print('Confusion Matrix:\\n', conf_mat)\n",
    "\n",
    "# display classification report\n",
    "print(classification_report(data_Y, y_pred))\n",
    "\n",
    "# fit the final model\n",
    "MLPSearch.fit(data_X,data_Y)\n",
    "print(MLPSearch.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "### Putting It to the Test\n",
    "##### New Records\n",
    "To rougly test the performance of this Neural Network model's ability to predict new records, I had two of my friends take the survey. Although they are both Star Wars fans (it's hard to get a variety during the quarantine), the model was able to predict both accurately based on their responses.\n",
    "##### Checking Weights\n",
    "As mentioned before, the features \"Location (Census Region)\" and \"Seen a Star Wars film\" were dropped from the dataset. That decision came from the results in this cell block, where we summed the entirety of each input node's weights to each node in the hidden layer (a sum of 106 weights for each feature). We printed which nodes were less than the average sum of weights, to get an idea of how widely these values ranged. Most values are close to the mean of ~46, however the two features mentioned above, had a total sum of weights less than 6. This means that the trained network had very low weights connecting these input nodes to each first layer hidden layer node. \n",
    "This implies that they had little to no effect on the classification, and for this reason, they were dropped at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10.207444480294264,\n",
       " 10.291860931078203,\n",
       " 10.029060369661481,\n",
       " 10.532192132306383,\n",
       " 9.797763645073752,\n",
       " 9.081594855196876,\n",
       " 10.42113109736299,\n",
       " 10.342573963853411,\n",
       " 8.883889304290838,\n",
       " 10.555974673364211,\n",
       " 9.45201475131237,\n",
       " 9.992752820487874,\n",
       " 10.114170110107299,\n",
       " 9.765772498114119,\n",
       " 10.407174265321636,\n",
       " 9.932844690230018,\n",
       " 9.5344082429577]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample record\n",
    "record = [[1,1,1,1,1,1,5,6,4,2,3,1,3,4,5,5,4,3,2,0,3,4,4,1,5,4,0,1,1,-1,1,1,5,3],\n",
    "         [1,1,1,1,1,1,5,6,4,2,3,1,5,4,4,3,4,1,4,3,0,5,5,1,4,5,0,-1,0,0,-1,1,4,3]]\n",
    "\n",
    "print(MLPSearch.predict(record))\n",
    "weights = MLPSearch.best_estimator_.steps[1][1].coefs_\n",
    "\n",
    "\n",
    "sum_of_weights = []\n",
    "for node in weights[0]:\n",
    "    node_sum = 0\n",
    "    for weight in node:\n",
    "        node_sum += abs(weight)\n",
    "    sum_of_weights.append(node_sum)\n",
    "    \n",
    "    \n",
    "avg = sum(sum_of_weights)/len(sum_of_weights)\n",
    "[weight for weight in sum_of_weights if weight < avg]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
